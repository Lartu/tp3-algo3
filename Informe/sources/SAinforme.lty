(pagebreak)


****3.4. Simulated Annealing****

***3.4.1. El algoritmo***
(par)
Simulated Annealing es una metaheurística que se utiliza para encontrar soluciones aproximadas a problemas que no poseen un algoritmo de resolución polinomial. Difieren de las heurísticas en que se basan en buscar la mejor solución de un conjunto de muchísimas soluciones posibles. Esta busqueda se puede realizar de muchas maneras, pero simulated Annealing utiliza una **búsqueda local**. A partir de una solución __S__ definimos un conjunto de soluciones relacionadas llamadas el vecindario de __S__. Revisamos estas soluciones y con algún criterio decidimos tomar una solución vecina __S'__ y calculamos su vecindario. Esto se repite hasta que decidamos dejar de buscar con otro criterio.  
(par)

(par)
El párrafo anterior parece ser un poco vago, ya que dejamos sin especificar muchas partes importantes de una búsqueda local como qué vecindario se utiliza, el criterio con el cual elegimos una solución y con cuál dejamos de buscar. Esto es intencional ya que hay una enorme cantidad de variantes posibles y para no perder generalidad debemos analizar cada caso por separado.
(par)

(par)
En el caso particular de Simulated Annealing buscamos una especie de híbrido entre explorar el conjunto de soluciones de manera de evitar los máximos locales (es decir, la mejor solución de un determinado conjunto de vecindarios que muy probablemente no sea la mejor solución global), pero aún así tender a seleccionar soluciones cada vez mejores para terminar encontrando una buena solución.
(par)

(par)
Para dar esta variabilidad seleccionaremos la próxima solución en base a un parámetro llamado Temperatura que comienza en un valor inicial (__Ts__) y decrece a medida que avanza la ejecución del programa. Este valor se utiliza como parámetro de una función de probabilidad P que decide si seleccionaremos una solución dada o no. Como regla general, se tiende a aceptar las soluciones mejores que la actual independientemente de la temperatura pero podemos aceptar soluciones peores si lo dicta nuestra función __P__. Como regla general, a menor temperatura es menor la probabilidad de selección de una solución inferior a la actual. También contamos con una función de energía E que nos permite comparar soluciones y en general depende del valor de la solución.
(par)

(par)
Cómo disminuye la temperatura a lo largo de la ejecución se denomina **Cooling Schedule** y es fundamental para el desempeño del algoritmo. Hay muchas opciones posibles que varían en efectividad dependiendo del tipo de problema y de las instancias particulares.
(par)

(par)
Veamos el pseudocódigo de un esquema de Simulated Annealing:
(par)

(todo) poner la imagen del pseudocodigo de las diapos (todo)

(par)
(todo)Esto asume que ya explicamos como está compuesta una solucion. O sea, que es un vector de vectores de clientes.(todo)
En nuestra implementación la función de vecindario que utilizaremos será **1-interchange**. El vecindario utilizando esta función está definido de la siguiente manera:
(par)

(todo)se puede escribir esto de manera mas linda (a nivel formato)(todo)

(par)
Sean S y S' soluciones. S' ∈ N(S) <=> ∃ i,j con i != j , i y j <= |rutas(S)|, ∃ c ∈ rutas(i), c != deposito tal que shift(rutas(S)[i],rutas(S)[j],c) == rutas(S') ∨ existe c1 ∈ rutas(i) ∧ c2 ∈ rutas(j), c1,c2 != deposito exchange(rutas(S)[i],rutas(S)[j],c1,c2) == rutas(S'). 
(par)

(par)
Siendo ``shift`` y ``exchange`` los siguientes procedimientos:
 
/codeblock.
shift(vector<Ruta> rutas ,Ruta ruta1, Ruta ruta2, cliente c, Grafo G)
    if (rutas[ruta1][c].demanda + Σ(demandas(ruta2)) <= G.capacidad_total)
	    w* <- w ∈ rutas[ruta2] / minArg(G.distanciaEntre(rutas[ruta1][c], rutas[ruta2][w]) + G.distanciaEntre(rutas[ruta2][w+1],rutas[ruta1][c]))
	    rutas[ruta2] = rutas[ruta2][1..w] ∪ rutas[ruta1][c] ∪ rutas[ruta2][w+1..|rutas[ruta2|-1]
	    rutas[ruta1] = rutas[ruta1] - c
    return rutas
.codeblock/


/codeblock. 
exchange(vector<Ruta> rutas, Ruta ruta1, Ruta ruta2,c1,c2, Grafo G)
	if(Σ(demandas(ruta1)) - rutas[ruta1][c1].demanda + rutas[ruta2][c2].demanda <= G.capacidad_total &&
	Σ(demandas(ruta2)) - rutas[ruta2][c2].demanda + rutas[ruta1][c1] < G.capacidad_total)
        temp = rutas[ruta1][c1]
        rutas[ruta1][c1] = rutas[ruta2][c2]
        rutas[ruta2][c2] = temp
    return rutas
.codeblock/

siendo shift un procedimiento tal que dadas dos rutas de la solución, remueve un cliente de una ruta y lo inserta en otra (siempre que la demanda del cliente insertado no exceda la capacidad del camión que recorre esa ruta) y exchange un swap entre dos clientes de dos rutas (siempre que las capacidades de los camiones de ambas rutas no sea excedido).
En la operación shift la inserción siempre se realizará de manera que la suma del costo de las aristas agregadas menos la arista removida sea mínimo.
(par)

(par)
Como **Cooling Schedule** tendremos varias opciones con diferente efectividad. Más adelante realizaremos experimentos comparándolos entre sí. Las variantes que consideraremos serán __CS0__:  Ti = Ts - i*(Ts - Tf)/ NIt, __CS1__: Ti = Ts * Tf/ Ts^(i/NIt) y __CS2__: Ti =  ((Ts-Tf)/(1+e^(0.3*(i - NIt/2)))) + Tf siendo __Ti__ la temperatura en la i-ésima iteración, __Ts__ la temperatura inicial, __Tf__ la temperatura final, __i__ el número de iteración y __NIt__ el total de iteraciones a realizar. Veamos a continuación gráficos de estas funciones para poder apreciarlas más tangiblemente:
(par)

(ps)
    (p33)
        (img)cs0.png(img)
        (caption)CS0(caption)
    (/p33)
    (p33)
        (img)cs1.png(img)
        (caption)CS1(caption)
    (/p33)
	(p33)
        (img)cs2.png(img)
        (caption)CS2(caption)
    (/p33)
(/ps)



(par)
Nuestra función de probabilidad será P = e^-Δ/Ti siendo Δ = E(S') - E(S) siendo E la función de energía de una solución que en nuestro caso corresponde al valor de la solución, es decir, la suma total de la distancia recorrida por cada camión. Notemos que a medida que Ti disminuye el valor de P también lo hace, cumpliendo la regla general "a menos temperatura, menos aceptación" mencionada previamente. En __P__ también tenemos en cuenta el valor de delta; si es negativo quiere decir que __S'__ es menor que __S__ y por lo tanto __P__ dará un número mayor que uno de manera que aceptaremos __S'__ como solución. En cambio si delta es positivo, su aceptación dependerá de su magnitud y de la temperatura, efectivamente cumpliendo que si __S'__ es considerablemente peor que __S__ sólo se acepte bajo altas temperaturas. 
(par)

(par)
Demos un pseudocódigo completo para nuestra implementación de Simulated Annealing:
(todo)Dar pseudocódigo completo (obviamente formal, pero usando P, CS y eso con sus valores reales)(todo)
(par)


***3.4.2. Analisis de complejidad***
(par)
(todo)explicarlo mejor con el pseudocódigo. No hay cota simple mas ajustada no?(todo) Dado que todas las operaciones se realizan en tiempo constante excepto shift que en su peor caso inserta en una ruta con todos los elementos menos uno, una cota simple del peor caso sería __n__ * __NIt__ donde n es la cantidad de nodos del grafo y __NIt__ es la cantidad de iteraciones. Es una cota brusca porque la probabilidad de inserciones en rutas grandes disminuye a medida que aumenta la cantidad de clientes (porque mientras más clientes tiene una ruta, menos probable es que siga teniendo espacio), pero dar una cota más ajustada es complejo porque depende de la instancia particular.
(par)

(todo)Mostrar los graficos que muestran que el algoritmo es lineal en funcion del nro de iteraciones(todo)

***3.4.3. Búsqueda de parámetros óptimos: caso aleatorio***

(par)
Busquemos el valor óptimo de los parámetros de nuestro algoritmo de Simulated Annealing para el casos aleatorio. Es decir, queremos encontrar el valor de los parámetros que mejores resultados obtiene en el promedio de muchos casos aleatorios, lo cual es útil para tener una noción general de qué valores tienden a obtener mejores resultados.
(par)

(par)
Para esto realizaremos una serie de experimentos, comenzando por un análisis de cómo el número de iteraciones realizadas (__Nit__) afecta el resultado. Sabemos que mientras más iteraciones realizemos mejor será el resultado ya que a fin de cuentas estamos realizando una búsqueda local, pero la pregunta que intentaremos responder es ¿Cuántas iteraciones tiene sentido medir? ¿Hay un punto a partir del cual la mejora obtenida de realizar más iteraciones no justifica el costo en tiempo de ejecución?
(par)

(todo)aclarar como fueron generadas las instancias, supongo que lo voy a aclarar una sola vez!(todo)

(par)
El experimento consistirá en medir el ahorro porcentual con respecto a la solución canónica  promedio obtenido luego de __Nit__ iteraciones para 400 instancias aleatorias del problema ), con un varios n fijos. Decidimos comparar con la solución canónica para poder comparar los resultados de diferentes n más claramente. La diferencia en efectividad observada utilizando las tres variantes de Cooling schedule dieron resultados muy similares, por lo que decidimos utilizar un único Cooling Schedule para la medición por practicidad. A continuación mostramos los resultados obtenidos:
(par)

(ps)
    (p50)
        (img)SANIt103.png(img)
        (caption)Ahorro porcentual de la solución canónica en función de NIt, __n__ = 103(caption)
    (/p50)
    (p50)
        (img)SANIt503.png(img)
        (caption)Ahorro porcentual de la solución canónica en función de NIt, __n__ = 503caption)
    (/p50)
(/ps)


(par)
Como podemos apreciar en los gráficos nuestras predicciones fueron correctas. A medida que aumenta __Nit__ el porcentaje de ahorro también aumenta. Notemos tambien que en ambos el pocentaje ahorrado aumenta rápidamente hasta las tres mil iteraciones y luego crece más lentamente. Esto se cumple en todos los grafos, pero a medida que aumenta el n el porcentaje ahorrado crece más rápidamente.
(par)

(par)
(todo)aclarar como son "las instancias"(todo)
Teniendo en cuenta las caracteristicas de las instancias que mediremos y que no disponemos de tiempo ilimitado para realizar las mediciones, decidimos que __Nit__ = 10.000 es suficientemente grande para ser representativo del resultado del algoritmo y suficientemente pequeño para poder realizar el resto de las mediciones en un tiempo razonable.
(par)

(par)
A continuación queremos averiguar el valor óptimo para la temperatura inicial Ts para cada Cooling Schedule. El experimento que realizaremos para encontrarlo será un similar al anterior; promediaremos el porcentaje de ahorro de 400 instancias para cada valor de temperatura inicial desde 2 hasta 202 (haciendo saltos de a 10), para cada Cooling Schedule. El tamaño de las instancias será de un n fijo (usaremos varios tamaños diferentes) y  __Nit__ = 10000 acorde a los resultados previos. Presentamos a continuación los resultados:
(par)

(ps)
    (p50)
        (img)SAVariandoTS(n=103).png(img)
    (/p50)
    (p50)
        (img)SAVariandoTs(n=503).png(img)
    (/p50)
(/ps)

(par)
Como podemos observar, la temperatura inicial Ts óptima para las tres alternativas de Cooling Schedule propuestas es Ts = 2 para todos los tamaños de grafo evaluados. Como parece respetarse esto independientemente del tamaño del grafo, podemos afirmar con bastante seguridad que Ts = 2 es óptimo.
(par)

(par)
Habiendo obtenido los valores óptimos para __Ts__ y __NIt__, realizemos la comparación entre el porcentaje de ahorro de las tres funciones Cooling Schedule en funcion del tamaño de la instancia. Realizando saltos de a 50, para cada n desde 3 hasta 1003 calculamos el ahorro promedio de 400 instancias de tamaño __n__, con __Ts__ = 2 y __NIt__ = 10.000. Estos son los resultados obtenidos:
(par)

(img)SAVariandoCS.png(img)

(par)
Se puede ver que __CS0__ y __CS1__ se comportan de manera identica excepto por los mayores n en los que __CS1__ resultó un poco mejor. __CS2__ resultó consistentemente en un peor ahorro, por lo que no sería nuestra elección de Cooling Schedule para una instancia aleatoria de la que no tenemos información.
(par)

(par)
Es importante destacar que a medida que aumenta n, todos las variantes de Cooling Schedule bajan en efectividad. Esto puede deberse al número de iteraciones, que como vimos anteriormente, debería ser mayor para dar mejores resultados en instancias de mayor tamaño.
(par)

(todo)Explicar como son las instancias del set A (a menos que se haga antes en el informe)(todo)

***3.4.4. Búsqueda de parámetros óptimos: Set A***

(par)
Busquemos ahora los parámetros óptimos para el set de instancias "Set A" propuesto por Augerat en 1995. Son instancias en las que se conoce el óptimo, por lo que en vez de medir los resultados como ahorro porcentual de la solución canónica los mediremos como distancia porcentual de la solución óptima; es decir, cuánto porciento mayor es nuestra solución comparada a la solución óptima. Esta manera de medir los resultados nos resulta más significativa que el ahorro porcentual de la solución canónica , que si bien es una medida aceptable para los casos aleatorios porque no tenemos información del óptimo, no es una medida de qué tan buena es la solución con respecto al mejor resultado posible.
(par)

(par)
La búsqueda de los parámetros óptimos se realizó calculando la distancia porcentual al óptimo para cada instancia del set, para cada variante de Cooling Schedule, para __NIt__ desde 1000 hasta 15000 con saltos de a 500, y con cada __Ts__ entero desde 2 hasta 100. Para cada configuración se calculó la suma del porcentaje ahorrado para cada instancia y se escogió la configuración que maximize esa suma. Elegimos la suma del porcentaje total ahorrado como medida de optimalidad por encima de la minimizacion de la suma directa de los resultados ya que esta última favorece configuraciones buenas para instancias de mayor tamaño. Con nuestra medida de optimalidad estamos valuando todas las instancias con el mismo nivel de importancia.
(par) 

(par)
La configuración óptima resultante fue como Cooling Schedule CS1, como temperatura inicial __Ts__ = 8 y número de iteraciones __NIt__ = 15000. Veamos a continuación la distancia porcentual al óptimo para cada instancia del set con la configuración óptima:
(par)

(img)SAdistOptimoA.png(img)

***3.4.5. Búsqueda de parámetros óptimos: Set X***

(par)
(todo)Explicar como son las instancias del set X (a menos que se haga antes en el informe)(todo)
Busquemos los parámetros óptimos para el set de instancias "Set X" propuesto por Uchoa et al. en 2014. Lo haremos de manera análoga a como lo hicimos para el Set A previamente.
(par)

(par)
Los resultados obtenidos fueron como Cooling Schedule __CS1__, __Ts__ = 20 y __NIt__ = 15000. Presentamos a continuación la distancia porcentual al óptimo para cada instancia del set con ésta configuración:
(par)

(img)SAdistOptimoX.png(img)

***3.4.5. Conclusiones de la experimentación***

(par)
De la búsqueda de parámetros óptimos para diferentes sets de instancias podemos extraer una serie de conclusiones acerca de como varía el comportamiento del algoritmo en función de los parámetros de entrada. En primer lugar es importante destacar el rol del número de iteraciones en el rendimiento del algoritmo. Si bien lo mencionamos previamente y es bastante sencillo entender su relevancia, no podemos dejarlo fuera de las conclusiones. No creemos que sea casualidad que los mejores resultados para todos los sets de instancias utilizados se obtuvieron para el mayor número de iteraciones probado. 
(par)

(par)
Sin embargo es importante destacar que si bien a mayor número de iteraciones se tiende a obtener mejores resultados, esto es una tendencia y no se cumple para todas las instancias. Buscando los parámetros óptimos para cada instancia encontramos muchos ejemplos en lo que no se cumplía. Esto no inválida la tendencia pero es importante tenerlo en cuenta a la hora de utilizar el algoritmo para la resolución de instancias. Si se utiliza una única instancia maximizar el número de iteraciones podría no resultar en el mejor rendimiento, pero si se quiere obtener el mejor rendimiento para una serie probablemente sí resulte en él. 
(par)

(par)
Otra conclusion es que el parámetro Ts afecta de manera distinta a la solución obtenida en función del Cooling Schedule que se utilize. Como podemos ver en el gráfico "Porcentaje de ahorro para diferentes valores de Ts", la variación de Ts afecta a los tres Cooling Schedule de manera muy distinta. CS0 decrece en efectividad linealmente en función de Ts, CS2 decrece rápidamente para algunos valores y luego aparenta mantenerse constante y CS1 no parece verse afectado en lo absoluto.
(par)

(par)
Es importante mencionar la baja efectividad del algoritmo para grafos de gran tamaño. Si bien los grafos de mayor tamaño necesitan un mayor número de iteraciones para dar soluciones de igual calidad que grafos menores (Como muestra la serie de gráficos "Porcentaje de ahorro para N iteraciones"), tenemos otra hipótesis acerca de este fenómenos. Esta es que el algoritmo en grafos de mayor tamaño es más propenso a diverger a malas soluciones dado que su vecindario es mayor y es menos probable dar pasos en la dirección del óptimo o de un vecindario de buenas soluciones. La raíz de este problema es que el algoritmo no tienen ninguna forma de volver hacía soluciones previas; una vez que da un paso, aunque sea un paso que empeore nuestra solución, no hay vuelta atrás.
(par)

(par)
Una alternativa para solucionar este inconveniente es implementar Resets o reinicios. La idea general es tener un criterio de reinicio que nos indica cuándo es momento de volver a una solución previa, ya que la actual es mucho peor que ésta. Generalmente la solución a la que se vuelve es la mejor solución que hemos encontrado en toda la ejecución del algoritmo hasta este punto. Esto es meramente una hipótesis y debería experimentarse y confirmarse debidamente para poder hacer afirmaciones al respecto.
(par)
